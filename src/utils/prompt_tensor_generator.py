import sys
sys.path.append("..")
import constants as C
import torch 
from PIL import Image
from transformers import pipeline
from transformers import AutoTokenizer, AutoModel
import argparse
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from torch import Tensor
from scipy.stats import norm
import utils as U


plms = ["bert", "biovil_t", "cxr_bert_s", "cxr_bert_g", "CLIP", "biomedCLIP"]

device = "cuda" if torch.cuda.is_available() else "cpu"
class PLM_embedding:
  '''
  draw heatmap based on the cosine sim. between embeddings generated by Pre-trained Language Model (PLM)
  '''
  def __init__(self):
    self.context_length = 256
    return
    
  def get_clip_text_embeddings(self, text:list):
    import clip
    model, preprocess = clip.load("ViT-B/32", device=device)
    text = clip.tokenize(text).to(device)
    with torch.no_grad():
        text_features = model.encode_text(text)
    del text
    return text_features
  
  def get_biomedclip_text_embedding(self, text:list):
    from open_clip import create_model_from_pretrained, get_tokenizer 
    image = r"..\imgs\chest.png"
    model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
    model.to(device)
    tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')
    texts = tokenizer(text, context_length=self.context_length).to(device)
    dummy_img = torch.stack([preprocess(Image.open(image))]).to(device)
    with torch.no_grad():
        _, text_features, _ = model(dummy_img, texts)
    return text_features
  
  def get_biovil_text_embedding(self, text_prompts:list):
    import torch
    from transformers import AutoModel, AutoTokenizer
    # Load the model and tokenizer
    url = "microsoft/BiomedVLP-BioViL-T"
    tokenizer = AutoTokenizer.from_pretrained(url, trust_remote_code=True)
    model = AutoModel.from_pretrained(url, trust_remote_code=True)

    with torch.no_grad():
        tokenizer_output = tokenizer.batch_encode_plus(batch_text_or_text_pairs=text_prompts,
                                                      add_special_tokens=True,
                                                      padding='longest',
                                                      return_tensors='pt')
        embeddings = model.get_projected_text_embeddings(input_ids=tokenizer_output.input_ids,
                                                    attention_mask=tokenizer_output.attention_mask)
        return embeddings
  
  def get_biovil_CXR_BERT_specialized_text_embedding(self, text_prompts:list):
    url = "microsoft/BiomedVLP-CXR-BERT-specialized"
    tokenizer = AutoTokenizer.from_pretrained(url, trust_remote_code=True)
    model = AutoModel.from_pretrained(url, trust_remote_code=True)

    # Tokenize and compute the sentence embeddings
    with torch.no_grad():
      tokenizer_output = tokenizer.batch_encode_plus(batch_text_or_text_pairs=text_prompts,
                                                  add_special_tokens=True,
                                                  padding='longest',
                                                  return_tensors='pt')
      embeddings = model.get_projected_text_embeddings(input_ids=tokenizer_output.input_ids,
                                                    attention_mask=tokenizer_output.attention_mask)
    return embeddings
  
  def get_biovil_CXR_BERT_general_text_embedding(self, text_prompts:list):
    '''
    First, we pretrain CXR-BERT-general from a randomly initialized BERT model via Masked Language Modeling (MLM) on abstracts 
    PubMed and clinical notes from the publicly-available MIMIC-III and MIMIC-CXR. 
    In that regard, the general model is expected be applicable for research in clinical domains other than the chest radiology 
    through domain specific fine-tuning.
    
    using average embedding in the sequence to represent sentence embedding
    '''
    from transformers import AutoTokenizer, AutoModelForMaskedLM
    tokenizer = AutoTokenizer.from_pretrained("microsoft/BiomedVLP-CXR-BERT-general")
    model = AutoModelForMaskedLM.from_pretrained("microsoft/BiomedVLP-CXR-BERT-general")
    with torch.no_grad():
      encoded_input = tokenizer(text_prompts, return_tensors='pt', padding=True, )
      output = model(**encoded_input, output_hidden_states=True)
    del encoded_input
    del model
    del tokenizer
    avg_embedding = torch.mean(output.hidden_states[-1], axis=1)
    del output
    return avg_embedding
  
  def get_bert_text_embedding(self, text_prompts:list):
    from transformers import BertTokenizer, BertModel
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = BertModel.from_pretrained("bert-base-uncased")
    with torch.no_grad():
      encoded_input = tokenizer(text_prompts, return_tensors='pt', padding=True)
      output = model(**encoded_input)
    cls_embedding = (output.pooler_output)
    '''
    Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining
    '''
    del encoded_input
    del output
    return cls_embedding

  def get_embedding(self, text:list, plm_name = None):
    if plm_name is None:
      raise ValueError("you have to specify the plm you want to use!!!")
    plms = {
        'clip': self.get_clip_text_embeddings,
        'bert': self.get_bert_text_embedding,
        'biomedclip': self.get_biomedclip_text_embedding,
        'biovil_t': self.get_biovil_text_embedding,
        'cxr_bert_s': self.get_biovil_CXR_BERT_specialized_text_embedding,
        'cxr_bert_g': self.get_biovil_CXR_BERT_general_text_embedding
    }
    
    if plm_name not in plms:
      raise ValueError(f"only support models: {list(plms.keys())}. you are using {plm_name}!!!")
    
    model = plms[plm_name]
    embedding = model(text)
    return embedding
    
def main():
    parser = argparse.ArgumentParser(description="generate prompt embedding based on different PLMs")
    parser.add_argument("--backbone", '-b',  type=str, help="specify the embedding model.", choices = ["clip", 'biomedclip', "biovil_t", "cxr_bert_s","cxr_bert_g", "bert"], required=True)
    parser.add_argument("--template", '-t',  type=str, default='basic', choices = ["detailed", "basic"],help="specify the prompt template(default: basic).")
    parser.add_argument("--saving_f", '-s',  type=str, default='./data/prompts_tensors' ,help="specify the folder to save embedding.")
    args = parser.parse_args()
    backbone = args.backbone
    template = args.template
    if template == "basic":
      text = C.BASIC_PROMPT
    elif template == "detailed":
      text = list(C.DESC_PROMPT.values())
    saving_f = args.saving_f
    saving_f += f"/{template}"
    
    generator = PLM_embedding()
    prompt_embedding = generator.get_embedding(text = text, plm_name = backbone)
    prompt_embedding /= prompt_embedding.norm(dim=-1, keepdim=True)
    
    tool = U.tools()
    tool.mkdir(saving_f)
    
    saving_name = f"{backbone}.pt"
    print(saving_f+"/"+saving_name)
    torch.save(prompt_embedding, saving_f+"/"+saving_name)
  
  
if __name__ == "__main__":
  main()
    
  
  