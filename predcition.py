import torch
from models import MultiTaskModel   
import constants as _constants_
import  matplotlib.pyplot as plt
import umap
import numpy as np


def plot(get_text_embedding, prompt, title = None):
  text_features = get_text_embedding
  similarity_matrix = torch.mm(text_features, text_features.t())
  text_features = text_features / text_features.norm(dim=-1, keepdim=True)
  # text_features = text_features.detach().numpy()
  similarity = (1 * text_features @ text_features.T)
  print(similarity.shape)
  similarity=similarity.cpu().detach().numpy()
  # data = similarity_matrix.cpu().numpy()
  # data.shape

  plt.figure(figsize=(8, 5))
  plt.imshow(similarity)
  plt.colorbar()
  plt.yticks(range(13), prompt, fontsize=7)
  plt.xticks(range(13), prompt, fontsize=7, rotation=40)
  # for i, image in enumerate(text):
  #     plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin="lower")
  for x in range(similarity.shape[1]):
      for y in range(similarity.shape[0]):
          plt.text(x, y, f"{similarity[y, x]:.2f}", ha="center", va="center", size=8)

  for side in ["left", "top", "right", "bottom"]:
    plt.gca().spines[side].set_visible(False)

  # plt.xlim([-0.5, 4 - 0.5])
  # plt.ylim([4 + 0.5, -2])
  print(len(similarity))
  avg_sim = (np.sum(similarity) - len(similarity))/(len(similarity)*(len(similarity)-1))
  title = "Cosine similarity between text and text features CLIP TEXT encoder" if title is None else title
  plt.title(title + f" : avg_sim: {avg_sim}", size=20)
  # plot.show()

model = MultiTaskModel(nntype="biomedclip")

# the model class: 你所定义的模型的class
model.load_state_dict(torch.load(r"D:\exchange\ShanghaiTech\learning\code\diagnosisP\x_ray_constrastive\output\checkpoint_11_11_bio\best\pytorch_model.bin"))
(model.eval())

prompt = _constants_.BASIC_PROMPT
image_path = ["D:\\exchange\\ShanghaiTech\\learning\\code\\diagnosisP\\x_ray_constrastive\\imgs\\structure.png"]
label = torch.tensor([1,1,2,1,2,1,1,2,1,0,0,2,1])
model.cuda()
contrastive_mode, classifier, orthogonal = model(prompt, image_path, label)
print(contrastive_mode.keys(), _constants_.RED + "constrastive_loss: "+str(contrastive_mode["loss_value"])+_constants_.RESET)
print(classifier.keys() ,_constants_.RED + "classifier_loss:"+str(classifier["loss_value"])+_constants_.RESET)
print(orthogonal.keys() , _constants_.RED + "orthogonal_loss: "+str(orthogonal["loss_value"])+ _constants_.RESET)

text_embedding = orthogonal['text_embeds']
print(text_embedding.shape)
text_embedding = text_embedding.squeeze()
print(text_embedding.shape)

reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=2, random_state=42)
embedding = reducer.fit_transform(text_embedding.cpu().detach().numpy())

# 提取降维后的坐标
x = embedding[:, 0]
y = embedding[:, 1]
labels = _constants_.BASIC_PROMPT

plt.scatter(x, y)
for i in range(len(labels)):
    plt.annotate(labels[i], (x[i], y[i]))  # 在点旁边添加标签

plt.title("UMAP Projection of Data with Labels using the data of trained model")
plt.grid()

plot(text_embedding.cpu(),prompt, "sim between text embedding generated by FG+CLIP")
plt.show()
# import clip
# import constants as _constants_
# import torch
# from PIL import Image
# import numpy as np
# import matplotlib.pyplot as plt
# import umap


# class print_plot_CLIP():
#   def __init__(self):
#     self.prompt = _constants_.BASIC_PROMPT

#   def get_text_embedding(self):
#     device = "cuda" if torch.cuda.is_available() else "cpu"
#     model, preprocess = clip.load("ViT-B/32", device=device)

#     text_inputs = torch.cat([clip.tokenize(f"{c}") for c in self.prompt]).to(device)

#     # Calculate features
#     print(text_inputs.shape)
#     with torch.no_grad():
#         text_features = model.encode_text(text_inputs)

#     print(text_features.size())
#     return text_features

#   def plot(self):
#     text_features = self.get_text_embedding()
#     similarity_matrix = torch.mm(text_features, text_features.t())
#     text_features /= text_features.norm(dim=-1, keepdim=True)
#     similarity = (1 * text_features @ text_features.T)
#     print(similarity.shape)
#     similarity=similarity.cpu().numpy()
#     data = similarity_matrix.cpu().numpy()
#     data.shape

#     plt.figure(figsize=(8, 5))
#     plt.imshow(similarity)
#     plt.colorbar()
#     plt.yticks(range(13), self.prompt, fontsize=7)
#     plt.xticks(range(13), self.prompt, fontsize=7, rotation=40)
#     # for i, image in enumerate(text):
#     #     plt.imshow(image, extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin="lower")
#     for x in range(similarity.shape[1]):
#         for y in range(similarity.shape[0]):
#             plt.text(x, y, f"{similarity[y, x]:.2f}", ha="center", va="center", size=8)

#     for side in ["left", "top", "right", "bottom"]:
#       plt.gca().spines[side].set_visible(False)

#     # plt.xlim([-0.5, 4 - 0.5])
#     # plt.ylim([4 + 0.5, -2])

#     plt.title("Cosine similarity between text and text features CLIP TEXT encoder", size=20)
#     similarity

#   def plot_UMAP(self):
#     text_features = self.get_text_embedding()
#     random_seed = 42
#     reducer = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3, random_state=random_seed)
#     from sklearn.preprocessing import StandardScaler  # 导入标准化工具
#     scaler = StandardScaler()
#     text_embedding = scaler.fit_transform(text_features.cpu().detach().numpy())

#     embedding = reducer.fit_transform(text_embedding)

#     # 计算均值
#     mean_x = np.mean(embedding[:, 0])
#     mean_y = np.mean(embedding[:, 1])
#     mean_z = np.mean(embedding[:, 2])

#     # 平移所有点，使原点位于中心
#     embedding_centered = embedding - [mean_x, mean_y, mean_z]

#     fig = plt.figure(figsize=(19, 13))
#     ax = fig.add_subplot(111, projection='3d')

#     ax.set_title("UMAP in original CLIP text embedding")

#     # 使用 ax.quiver 绘制三维向量
#     for i in range(len(embedding)):
#         x_start = 0  # 起点 x 坐标
#         y_start = 0  # 起点 y 坐标
#         z_start = 0  # 起点 z 坐标

#         x_vector = embedding_centered[i, 0]  # x 方向上的分量
#         y_vector = embedding_centered[i, 1]  # y 方向上的分量
#         z_vector = embedding_centered[i, 2]  # z 方向上的分量

#         # ax.quiver(x_start, y_start, z_start, x_vector, y_vector, z_vector)
#         color = plt.cm.viridis((i) / len(embedding_centered))  # 根据索引设置颜色
#         ax.quiver(x_start, y_start, z_start, x_vector, y_vector, z_vector, color=color, label=self.prompt[i])

#         print(x_vector, y_vector, z_vector)

#         # ax.text(embedding[i, 0], embedding[i, 1], embedding[i, 2], prompt[i])
#     max_range = np.max(np.abs(embedding_centered))
#     ax.set_xlim([-max_range, max_range])
#     ax.set_ylim([-max_range, max_range ])
#     ax.set_zlim([-max_range, max_range])
#     ax.legend()
#     # plt.grid()
#     plt.show()
   

# a = print_plot_CLIP()

# a.plot()
# a.plot_UMAP()