I have tried different Pre-trained Language Models(PLMs), and make an exploration of the embedding of different prompts generated by different PLM.

- CLIP -- text encoder
- BiomedCLIP -- text encoder
- BioViL -- text encoder


this table is the specific models trained in different dataset.

| Model                 | Model Identifier                           | Vocabulary     | Note                                             |
|-----------------------|--------------------------------------------|-----------------|--------------------------------------------------|
| CXR-BERT-general      | `microsoft/BiomedVLP-CXR-BERT-general`     | PubMed & MIMIC  | Pretrained for biomedical literature and clinical domains |
| CXR-BERT-specialized  | `microsoft/BiomedVLP-CXR-BERT-specialized` | PubMed & MIMIC  | Static pretraining for the CXR domain              |
| BioViL-T              | `microsoft/BiomedVLP-BioViL-T`             | PubMed & MIMIC  | Static & temporal pretraining for the CXR domain  |
